{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the ptb dataset for testing\n",
    "# copy of tensorflow examples\n",
    "\n",
    "def _read_words(filename):\n",
    "  with tf.gfile.GFile(filename, \"r\") as f:\n",
    "    return f.read().decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "\n",
    "def _build_vocab(filename):\n",
    "  data = _read_words(filename)\n",
    "\n",
    "  counter = collections.Counter(data)\n",
    "  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "  words, _ = list(zip(*count_pairs))\n",
    "  word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "  return word_to_id\n",
    "\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id):\n",
    "  data = _read_words(filename)\n",
    "  return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n",
    "def ptb_raw_data(data_path=None):\n",
    "  \"\"\"Load PTB raw data from data directory \"data_path\".\n",
    "  Reads PTB text files, converts strings to integer ids,\n",
    "  and performs mini-batching of the inputs.\n",
    "  The PTB dataset comes from Tomas Mikolov's webpage:\n",
    "  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "  Args:\n",
    "    data_path: string path to the directory where simple-examples.tgz has\n",
    "      been extracted.\n",
    "  Returns:\n",
    "    tuple (train_data, valid_data, test_data, vocabulary)\n",
    "    where each of the data objects can be passed to PTBIterator.\n",
    "  \"\"\"\n",
    "\n",
    "  train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "  valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "  test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "\n",
    "  word_to_id = _build_vocab(train_path)\n",
    "  train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "  valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "  test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "  vocabulary = len(word_to_id)\n",
    "  return train_data, valid_data, test_data, vocabulary\n",
    "\n",
    "\n",
    "def ptb_producer(raw_data, batch_size, num_steps, name=None):\n",
    "  \"\"\"Iterate on the raw PTB data.\n",
    "  This chunks up raw_data into batches of examples and returns Tensors that\n",
    "  are drawn from these batches.\n",
    "  Args:\n",
    "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
    "    batch_size: int, the batch size.\n",
    "    num_steps: int, the number of unrolls.\n",
    "    name: the name of this operation (optional).\n",
    "  Returns:\n",
    "    A pair of Tensors, each shaped [batch_size, num_steps]. The second element\n",
    "    of the tuple is the same data time-shifted to the right by one.\n",
    "  Raises:\n",
    "    tf.errors.InvalidArgumentError: if batch_size or num_steps are too high.\n",
    "  \"\"\"\n",
    "  with tf.name_scope(name, \"PTBProducer\", [raw_data, batch_size, num_steps]):\n",
    "    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "\n",
    "    data_len = tf.size(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    data = tf.reshape(raw_data[0 : batch_size * batch_len],\n",
    "                      [batch_size, batch_len])\n",
    "\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    assertion = tf.assert_positive(\n",
    "        epoch_size,\n",
    "        message=\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "    with tf.control_dependencies([assertion]):\n",
    "      epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\n",
    "\n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "    x = tf.strided_slice(data, [0, i * num_steps], [-1,-1],\n",
    "                         [batch_size, (i + 1) * num_steps])\n",
    "    x.set_shape([batch_size, num_steps])\n",
    "    y = tf.strided_slice(data, [0, i * num_steps + 1], [-1,-1],\n",
    "                         [batch_size, (i + 1) * num_steps + 1])\n",
    "    y.set_shape([batch_size, num_steps])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, val, test, vocab = ptb_raw_data('/home/kaushik/Desktop/timeseriesDL/data/ptb')\n",
    "\n",
    "x_batch, y_batch = ptb_producer(train, 64, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'PTBProducer/StridedSlice:0' shape=(64, 20) dtype=int32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import sys, path\n",
    "sys.path.append(path.dirname(path.abspath('.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from layers import lstmLayer, DeepLSTM\n",
    "\n",
    "# TODO: add batch re-norm and dropout\n",
    "class LSTM(object):\n",
    "    '''Class defining the overall model based on layers.py'''\n",
    "    def __init__(self, args):\n",
    "\n",
    "        self.seq_len = args['seq_len']\n",
    "        self.num_layers = args['num_layers']\n",
    "        self.cell = args['cell']\n",
    "        self.hidden_units = args['hidden_units']\n",
    "        self.op_classes = args['op_channels']\n",
    "        self.mode = args['mode']\n",
    "        self.init_lr = args['lr_rate']\n",
    "        self.grad_clip = args['grad_clip']\n",
    "        self.batch_size = args['batch_size']\n",
    "\n",
    "    def build_graph(self):\n",
    "\n",
    "        self._build_model()\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            self._add_train_nodes()\n",
    "        self.summaries = tf.summary.merge_all()\n",
    "\n",
    "    def _build_model(self):\n",
    "\n",
    "        # define placeholder for data layer\n",
    "        self.input_layer_x = tf.placeholder(dtype=tf.float32, shape=[self.batch_size, self.seq_len],name='input_layer_x')\n",
    "\n",
    "        # define model based on cell and num_layers\n",
    "        if self.num_layers ==1:\n",
    "          self.lstm_layer = lstmLayer(self.hidden_units)\n",
    "        else:\n",
    "          cells = [lstmLayer(self.hidden_units)]*self.num_layers\n",
    "          self.lstm_layer = DeepLSTM(cells)\n",
    "\n",
    "        # TODO: Think about the need for statefullness in this problem scenario\n",
    "        self.initial_state = tf.zeros([self.batch_size,self.lstm_layer.state_size],tf.float32)\n",
    "        #self.initial_state = tf.placeholder(tf.float32,[None, self.lstm_layer.state_size])\n",
    "\n",
    "        state = self.initial_state\n",
    "        output = tf.zeros([self.batch_size,self.hidden_units],dtype=tf.float32)\n",
    "        # run the model for multiple time steps\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "          for time in range(self.seq_len):\n",
    "            if time > 0: tf.get_variable_scope().reuse_variables()\n",
    "            # pass the inputs, state and weather we are in train/test or inference time (for dropout)\n",
    "            output, state = self.lstm_layer(self.input_layer_x[:,time], state)\n",
    "\n",
    "        self.final_state = state\n",
    "        self.final_output = output\n",
    "\n",
    "        softmax_w = tf.get_variable('softmax_w', [self.hidden_units, self.op_classes],dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer())\n",
    "        softmax_b = tf.get_variable('softmax_b', [self.op_classes],dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer())\n",
    "\n",
    "        self.logits = tf.matmul(self.final_output, softmax_w) + softmax_b\n",
    "\n",
    "        # get probabilities for these logits through softmax (will be needed for sampling)\n",
    "        self.output_prob = tf.nn.softmax(self.logits)\n",
    "        activation_summary(self.output_prob)\n",
    "\n",
    "    def _add_train_nodes(self):\n",
    "\n",
    "        # define placeholder for target layer\n",
    "        self.input_layer_y = tf.placeholder(tf.float32, [self.batch_size,self.op_classes],name='input_layer_y')\n",
    "\n",
    "        # sequence loss by example\n",
    "        # TODO: Implement proper loss function for encoder like structure of LSTM\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.logits,self.input_layer_y))\n",
    "        #self.cost = weighted_cross_entropy(self.class_weights,self.logits,self.input_layer_y)\n",
    "        tf.summary.scalar(\"loss\",self.cost)\n",
    "\n",
    "        self.lr = tf.Variable(self.init_lr, trainable=False)\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        grads_vars = optimizer.compute_gradients(self.cost,trainable_variables)\n",
    "\n",
    "        # histogram_summaries for weights and gradients\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "        for grad, var in grads_vars:\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(var.op.name+'/gradient',grad)\n",
    "\n",
    "        # TODO: Think about how gradient clipping is implemented, cross check\n",
    "        grads, _ = tf.clip_by_global_norm([grad for (grad,var) in grads_vars], self.grad_clip)\n",
    "        self.train_op = optimizer.apply_gradients(grads_vars)\n",
    "\n",
    "    def assign_lr(self,session,lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    def initialize_state(self,session):\n",
    "        session.run(self.initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bi-directional LSTM code based on LSTM code\n",
    "\n",
    "class BiLSTM(object):\n",
    "    \n",
    "    def __init__(self,args):\n",
    "        \n",
    "        self.seq_len = args['seq_len']\n",
    "        self.num_layers = args['num_layers']\n",
    "        self.cell = args['cell']\n",
    "        self.hidden_units = args['hidden_units']\n",
    "        self.op_classes = args['op_channels']\n",
    "        self.mode = args['mode']\n",
    "        self.init_lr = args['lr_rate']\n",
    "        self.grad_clip = args['grad_clip']\n",
    "        self.batch_size = args['batch_size']\n",
    "        \n",
    "    def build_graph():\n",
    "        \n",
    "        self._build_model()\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            self._add_training_nodes()\n",
    "        self.summaries = tf.summary.merge_all()\n",
    "        \n",
    "        \n",
    "    def _build_model(self):\n",
    "        \n",
    "        # define placeholder for data layer\n",
    "        self.input_layer_x = tf.placeholder(dtype=tf.int32, shape=[self.batch_size, self.seq_len],name='input_layer_x')\n",
    "\n",
    "        # define model based on cell and num_layers\n",
    "        self.fw_lstm_layer = lstmLayer(self.hidden_units)\n",
    "        self.bk_lstm_layer = lstmLayer(self.hidden_units)\n",
    "\n",
    "        self.initial_state = tf.zeros([self.batch_size,self.fw_lstm_layer.state_size],tf.float32)\n",
    "        #self.initial_state = tf.placeholder(tf.float32,[None, self.lstm_layer.state_size])\n",
    "\n",
    "        def _run_loop(input_data,name):\n",
    "            state = self.initial_state\n",
    "            output = tf.zeros([self.batch_size,self.hidden_units],dtype=tf.float32)\n",
    "            outputs = []\n",
    "            # run the model for multiple time steps\n",
    "            with tf.variable_scope(name+\"LSTM\"):\n",
    "              for time in range(self.seq_len):\n",
    "                if time > 0: tf.get_variable_scope().reuse_variables()\n",
    "                # pass the inputs, state and weather we are in train/test or inference time (for dropout)\n",
    "                output, state = self.fw_lstm_layer(input_data[:,time], state)\n",
    "                outputs.append(output)\n",
    "                \n",
    "            return outputs\n",
    "        \n",
    "        # run the forward chain\n",
    "        fw_outputs = _run_loop(self.input_layer_x,'fw_')\n",
    "        # run the backward chain\n",
    "        bk_input_x = tf.reverse(self.input_layer_x, [False, True])\n",
    "        bk_ouputs = _run_loop(bk_input_x,'bk_')[::-1]\n",
    "        \n",
    "        # concat the forward and backward runs\n",
    "        self.final_ouput = tf.zeros([self.seq_len,self.batch_size,2*self.hidden_units],dtype=tf.float32)\n",
    "        for it, (fw,bk) in enumerate(zip(fw_outputs,bk_outputs)):\n",
    "            self.final_ouput[it,:,:] = tf.concat(1,[fw,bk])\n",
    "        \n",
    "        # now combine with softmax to produce output\n",
    "        softmax_w = tf.get_variable('softmax_w', [2*self.hidden_units, self.op_classes],dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer())\n",
    "        softmax_b = tf.get_variable('softmax_b', [self.op_classes],dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer())\n",
    "\n",
    "        self.logits = tf.matmul(self.final_output, softmax_w) + softmax_b\n",
    "\n",
    "        # get probabilities for these logits through softmax (will be needed for sampling)\n",
    "        self.output_prob = tf.nn.softmax(self.logits)\n",
    "        activation_summary(self.output_prob)\n",
    "\n",
    "    def _add_train_nodes(self):\n",
    "\n",
    "        # define placeholder for target layer\n",
    "        self.input_layer_y = tf.placeholder(tf.float32, [self.batch_size,self.op_classes],name='input_layer_y')\n",
    "\n",
    "        # sequence loss by example\n",
    "        # TODO: Implement proper loss function for encoder like structure of LSTM\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.logits,self.input_layer_y))\n",
    "        #self.cost = weighted_cross_entropy(self.class_weights,self.logits,self.input_layer_y)\n",
    "        tf.summary.scalar(\"loss\",self.cost)\n",
    "\n",
    "        self.lr = tf.Variable(self.init_lr, trainable=False)\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        grads_vars = optimizer.compute_gradients(self.cost,trainable_variables)\n",
    "\n",
    "        # histogram_summaries for weights and gradients\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "        for grad, var in grads_vars:\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(var.op.name+'/gradient',grad)\n",
    "\n",
    "        # TODO: Think about how gradient clipping is implemented, cross check\n",
    "        grads, _ = tf.clip_by_global_norm([grad for (grad,var) in grads_vars], self.grad_clip)\n",
    "        self.train_op = optimizer.apply_gradients(grads_vars)\n",
    "\n",
    "    def assign_lr(self,session,lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    def initialize_state(self,session):\n",
    "        session.run(self.initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0cf53467a29a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-118ece5bc9b1>\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-118ece5bc9b1>\u001b[0m in \u001b[0;36m_build_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m# pass the inputs, state and weather we are in train/test or inference time (for dropout)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kaushik/Desktop/timeseriesDL/layers.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_data, state, scope)\u001b[0m\n\u001b[1;32m    245\u001b[0m                     \u001b[0mcurr_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                     \u001b[0;31m# hidden unit is propagated as the input_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                     \u001b[0mcurr_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m                     \u001b[0mnew_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcurr_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kaushik/Desktop/timeseriesDL/layers.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_data, state, scope)\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mip2hidden\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden2hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mip_gate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'input_gate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0mip_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'input_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mforget_gate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'forget_gate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kaushik/Desktop/timeseriesDL/layers.pyc\u001b[0m in \u001b[0;36msum_inputs\u001b[0;34m(input_data, h, scope)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                     ip2hiddenW = tf.get_variable('ip2hidden',\n\u001b[0;32m--> 195\u001b[0;31m                                                  \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m                                                  dtype=tf.float32,initializer=tf.random_uniform_initializer())\n\u001b[1;32m    197\u001b[0m                     hidden2hiddenW = tf.get_variable('hidden2hidden',\n",
      "\u001b[0;32m/home/kaushik/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "args = {'num_epochs': 5, 'lr_rate': 0.01, 'lr_decay': 0.97, 'seq_len': 20, 'num_layers': 2, 'cell': 'lstm', \n",
    "        'hidden_units': 64, 'op_channels': 10000,'grad_clip': 5.0, 'batch_size': 64}\n",
    "\n",
    "# Initialize session and graph\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    \n",
    "    args['mode'] = 'train'\n",
    "\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "\n",
    "        train_model = LSTM(args)\n",
    "        \n",
    "\n",
    "        train_model.build_graph()\n",
    "        tf.initialize_all_variables().run()\n",
    "\n",
    "        cost_over_batches = []\n",
    "\n",
    "        for i in range(args['num_epochs']):\n",
    "            lr_decay = args['lr_decay'] ** max(i - 2.0, 0.0)\n",
    "            train_model.assign_lr(session, args['lr_rate'] * lr_decay)\n",
    "\n",
    "            start_time = time.time()\n",
    "            softmax_op = np.zeros((max_batches*model.batch_size,model.op_channels))\n",
    "            cost_trajectory = []\n",
    "            y_onehot = np.zeros((max_batches*model.batch_size,model.op_channels))\n",
    "            epoch_cost = 0.0\n",
    "\n",
    "            for i in range(max_batches):\n",
    "                x, y = ptb_producer(train, 64, 20)\n",
    "                summary, cur_cost, output_prob, _ = session.run([model.summaries,model.cost,model.output_prob,model.train_op],\n",
    "                            feed_dict={model.input_layer_x: x, model.input_layer_y: y})\n",
    "                cost_trajectory.append(cur_cost)\n",
    "                softmax_op[i*len(y):(i+1)*len(y),:] = output_prob\n",
    "                y_onehot[i*len(y):(i+1)*len(y),:] = y\n",
    "                epoch_cost += cur_cost\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            lprint(\"Runtime of one epoch: \")\n",
    "            lprint(end_time-start_time)\n",
    "            lprint(\"Average cost per epoch: \")\n",
    "            lprint(epoch_cost/max_batches)\n",
    "\n",
    "            cost_over_batches += cost_trajectory\n",
    "\n",
    "        plt.plot(np.linspace(1,len(cost_over_batches),len(cost_over_batches)),cost_over_batches)\n",
    "        plt.title('Cost per batch over the training run')\n",
    "        plt.xlabel('# batch')\n",
    "        plt.ylabel('avg. cost per batch')\n",
    "        plt.savefig(logdir+'model_'+str(ix)+'_traincost.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
