{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 100000000\n",
      "(99999000, 'ons anarchists advocate social relations based upon voluntary as')\n",
      "(1000, ' anarchism originated as a term of abuse first used against earl')\n",
      "Unexpected character: ï\n",
      "(1, 26, 0, 0)\n",
      "('a', 'z', ' ')\n"
     ]
    }
   ],
   "source": [
    "# import the ptb dataset for testing\n",
    "# copy of tensorflow examples\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import string\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))\n",
    "\n",
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    \n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import sys, path\n",
    "sys.path.append(path.dirname(path.abspath('.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from layers import *\n",
    "\n",
    "\n",
    "# TODO: add batch re-norm and dropout\n",
    "class LSTM(object):\n",
    "    '''Class defining the overall model based on layers.py'''\n",
    "    def __init__(self, args):\n",
    "\n",
    "        self.seq_len = args['seq_len']\n",
    "        self.num_layers = args['num_layers']\n",
    "        self.cell = args['cell']\n",
    "        self.hidden_units = args['hidden_units']\n",
    "        self.op_classes = args['op_channels']\n",
    "        self.mode = args['mode']\n",
    "        self.init_lr = args['lr_rate']\n",
    "        self.grad_clip = args['grad_clip']\n",
    "        self.batch_size = args['batch_size']\n",
    "\n",
    "    def build_graph(self):\n",
    "\n",
    "        self._build_model()\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            self._add_train_nodes()\n",
    "        self.summaries = tf.summary.merge_all()\n",
    "\n",
    "    def _build_model(self):\n",
    "\n",
    "        # define placeholder for data layer\n",
    "        self.input_layer_x = tf.placeholder(dtype=tf.float32, shape=[self.batch_size, self.seq_len,1],name='input_layer_x')\n",
    "\n",
    "        # define model based on cell and num_layers\n",
    "        if self.num_layers ==1:\n",
    "          self.lstm_layer = lstmLayer(self.hidden_units)\n",
    "        else:\n",
    "          cells = [lstmLayer(self.hidden_units)]*self.num_layers\n",
    "          self.lstm_layer = DeepLSTM(cells)\n",
    "\n",
    "        # TODO: Think about the need for statefullness in this problem scenario\n",
    "        self.initial_state = tf.zeros([self.batch_size,self.lstm_layer.state_size],tf.float32)\n",
    "        #self.initial_state = tf.placeholder(tf.float32,[None, self.lstm_layer.state_size])\n",
    "\n",
    "        state = self.initial_state\n",
    "        output = tf.zeros([self.batch_size,self.hidden_units],dtype=tf.float32)\n",
    "        # run the model for multiple time steps\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "          for time in range(self.seq_len):\n",
    "            if time > 0: tf.get_variable_scope().reuse_variables()\n",
    "            # pass the inputs, state and weather we are in train/test or inference time (for dropout)\n",
    "            output, state = self.lstm_layer(self.input_layer_x[:,time,:], state)\n",
    "\n",
    "        self.final_state = state\n",
    "        self.final_output = output\n",
    "\n",
    "        softmax_w = tf.get_variable('softmax_w', [self.hidden_units, self.op_classes],dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer())\n",
    "        softmax_b = tf.get_variable('softmax_b', [self.op_classes],dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer())\n",
    "\n",
    "        self.logits = tf.matmul(self.final_output, softmax_w) + softmax_b\n",
    "\n",
    "        # get probabilities for these logits through softmax (will be needed for sampling)\n",
    "        self.output_prob = tf.nn.softmax(self.logits)\n",
    "        activation_summary(self.output_prob)\n",
    "\n",
    "    def _add_train_nodes(self):\n",
    "\n",
    "        # define placeholder for target layer\n",
    "        self.input_layer_y = tf.placeholder(tf.float32, [self.batch_size,self.op_classes],name='input_layer_y')\n",
    "\n",
    "        # sequence loss by example\n",
    "        # TODO: Implement proper loss function for encoder like structure of LSTM\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.logits,self.input_layer_y))\n",
    "        #self.cost = weighted_cross_entropy(self.class_weights,self.logits,self.input_layer_y)\n",
    "        tf.summary.scalar(\"loss\",self.cost)\n",
    "\n",
    "        self.lr = tf.Variable(self.init_lr, trainable=False)\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        grads_vars = optimizer.compute_gradients(self.cost,trainable_variables)\n",
    "\n",
    "        # histogram_summaries for weights and gradients\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "        for grad, var in grads_vars:\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(var.op.name+'/gradient',grad)\n",
    "\n",
    "        # TODO: Think about how gradient clipping is implemented, cross check\n",
    "        grads, _ = tf.clip_by_global_norm([grad for (grad,var) in grads_vars], self.grad_clip)\n",
    "        self.train_op = optimizer.apply_gradients(grads_vars)\n",
    "\n",
    "    def assign_lr(self,session,lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    def initialize_state(self,session):\n",
    "        session.run(self.initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bi-directional LSTM code based on LSTM code\n",
    "\n",
    "class BiLSTM(object):\n",
    "    \n",
    "    def __init__(self,args):\n",
    "        \n",
    "        self.seq_len = args['seq_len']\n",
    "        self.num_layers = args['num_layers']\n",
    "        self.cell = args['cell']\n",
    "        self.hidden_units = args['hidden_units']\n",
    "        self.op_classes = args['op_channels']\n",
    "        self.mode = args['mode']\n",
    "        self.init_lr = args['lr_rate']\n",
    "        self.grad_clip = args['grad_clip']\n",
    "        self.batch_size = args['batch_size']\n",
    "        \n",
    "    def build_graph():\n",
    "        \n",
    "        self._build_model()\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            self._add_training_nodes()\n",
    "        self.summaries = tf.summary.merge_all()\n",
    "        \n",
    "        \n",
    "    def _build_model(self):\n",
    "        \n",
    "        # define placeholder for data layer\n",
    "        self.input_layer_x = tf.placeholder(dtype=tf.int32, shape=[self.batch_size, self.seq_len,1],name='input_layer_x')\n",
    "\n",
    "        # define model based on cell and num_layers\n",
    "        self.fw_lstm_layer = lstmLayer(self.hidden_units)\n",
    "        self.bk_lstm_layer = lstmLayer(self.hidden_units)\n",
    "\n",
    "        self.initial_state = tf.zeros([self.batch_size,self.fw_lstm_layer.state_size],tf.float32)\n",
    "        #self.initial_state = tf.placeholder(tf.float32,[None, self.lstm_layer.state_size])\n",
    "\n",
    "        def _run_loop(input_data,name):\n",
    "            state = self.initial_state\n",
    "            output = tf.zeros([self.batch_size,self.hidden_units],dtype=tf.float32)\n",
    "            outputs = []\n",
    "            # run the model for multiple time steps\n",
    "            with tf.variable_scope(name+\"LSTM\"):\n",
    "              for time in range(self.seq_len):\n",
    "                if time > 0: tf.get_variable_scope().reuse_variables()\n",
    "                # pass the inputs, state and weather we are in train/test or inference time (for dropout)\n",
    "                output, state = self.fw_lstm_layer(input_data[:,time,:], state)\n",
    "                outputs.append(output)\n",
    "                \n",
    "            return outputs\n",
    "        \n",
    "        # run the forward chain\n",
    "        fw_outputs = _run_loop(self.input_layer_x,'fw_')\n",
    "        # run the backward chain\n",
    "        bk_input_x = tf.reverse(self.input_layer_x, [False, True])\n",
    "        bk_ouputs = _run_loop(bk_input_x,'bk_')[::-1]\n",
    "        \n",
    "        # concat the forward and backward runs\n",
    "        self.final_ouput = tf.zeros([self.seq_len,self.batch_size,2*self.hidden_units],dtype=tf.float32)\n",
    "        for it, (fw,bk) in enumerate(zip(fw_outputs,bk_outputs)):\n",
    "            self.final_ouput[it,:,:] = tf.concat(1,[fw,bk])\n",
    "        \n",
    "        # now combine with softmax to produce output\n",
    "        softmax_w = tf.get_variable('softmax_w', [2*self.hidden_units, self.op_classes],dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer())\n",
    "        softmax_b = tf.get_variable('softmax_b', [self.op_classes],dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer())\n",
    "\n",
    "        self.logits = tf.matmul(self.final_output, softmax_w) + softmax_b\n",
    "\n",
    "        # get probabilities for these logits through softmax (will be needed for sampling)\n",
    "        self.output_prob = tf.nn.softmax(self.logits)\n",
    "        activation_summary(self.output_prob)\n",
    "\n",
    "    def _add_train_nodes(self):\n",
    "\n",
    "        # define placeholder for target layer\n",
    "        self.input_layer_y = tf.placeholder(tf.float32, [self.batch_size,self.op_classes],name='input_layer_y')\n",
    "\n",
    "        # sequence loss by example\n",
    "        # TODO: Implement proper loss function for encoder like structure of LSTM\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.logits,self.input_layer_y))\n",
    "        #self.cost = weighted_cross_entropy(self.class_weights,self.logits,self.input_layer_y)\n",
    "        tf.summary.scalar(\"loss\",self.cost)\n",
    "\n",
    "        self.lr = tf.Variable(self.init_lr, trainable=False)\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        grads_vars = optimizer.compute_gradients(self.cost,trainable_variables)\n",
    "\n",
    "        # histogram_summaries for weights and gradients\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "        for grad, var in grads_vars:\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(var.op.name+'/gradient',grad)\n",
    "\n",
    "        # TODO: Think about how gradient clipping is implemented, cross check\n",
    "        grads, _ = tf.clip_by_global_norm([grad for (grad,var) in grads_vars], self.grad_clip)\n",
    "        self.train_op = optimizer.apply_gradients(grads_vars)\n",
    "\n",
    "    def assign_lr(self,session,lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    def initialize_state(self,session):\n",
    "        session.run(self.initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-40-f3c3bc1a1157>:16 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "{'grad_clip': 5.0, 'num_layers': 2, 'batch_size': 64, 'num_epochs': 5, 'lr_rate': 0.01, 'hidden_units': 64, 'op_channels': 10000, 'cell': 'lstm', 'mode': 'train', 'seq_len': 20, 'lr_decay': 0.97}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (64, 20, 1) for Tensor u'model/input_layer_y:0', which has shape '(64, 10000)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-f3c3bc1a1157>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 summary, cur_cost, output_prob, _ = session.run([model.summaries,model.cost,model.output_prob,model.train_op],\n\u001b[0;32m---> 35\u001b[0;31m                             feed_dict={model.input_layer_x: x, model.input_layer_y: y})\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mcost_trajectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0msoftmax_op\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kaushik/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kaushik/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    941\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    944\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (64, 20, 1) for Tensor u'model/input_layer_y:0', which has shape '(64, 10000)'"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "import time\n",
    "\n",
    "args = {'num_epochs': 5, 'lr_rate': 0.01, 'lr_decay': 0.97, 'seq_len': 20, 'num_layers': 2, 'cell': 'lstm', \n",
    "        'hidden_units': 64, 'op_channels': 10000,'grad_clip': 5.0, 'batch_size': 64}\n",
    "\n",
    "# Initialize session and graph\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    \n",
    "    args['mode'] = 'train'\n",
    "\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "\n",
    "        model = LSTM(args)\n",
    "        model.build_graph()\n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "        print args\n",
    "\n",
    "        cost_over_batches = []\n",
    "\n",
    "        for i in range(args['num_epochs']):\n",
    "            lr_decay = args['lr_decay'] ** max(i - 2.0, 0.0)\n",
    "            model.assign_lr(session, args['lr_rate'] * lr_decay)\n",
    "\n",
    "            start_time = time.time()\n",
    "            softmax_op = np.zeros((1000*model.batch_size,model.op_classes))\n",
    "            cost_trajectory = []\n",
    "            y_onehot = np.zeros((1000*model.batch_size,model.op_classes))\n",
    "            epoch_cost = 0.0\n",
    "\n",
    "            for i in range(1000):\n",
    "                 = train_batches.next()\n",
    "                summary, cur_cost, output_prob, _ = session.run([model.summaries,model.cost,model.output_prob,model.train_op],\n",
    "                            feed_dict={model.input_layer_x: x, model.input_layer_y: y})\n",
    "                cost_trajectory.append(cur_cost)\n",
    "                softmax_op[i*len(y):(i+1)*len(y),:] = output_prob\n",
    "                y_onehot[i*len(y):(i+1)*len(y),:] = y\n",
    "                epoch_cost += cur_cost\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            lprint(\"Runtime of one epoch: \")\n",
    "            lprint(end_time-start_time)\n",
    "            lprint(\"Average cost per epoch: \")\n",
    "            lprint(epoch_cost/max_batches)\n",
    "\n",
    "            cost_over_batches += cost_trajectory\n",
    "\n",
    "        plt.plot(np.linspace(1,len(cost_over_batches),len(cost_over_batches)),cost_over_batches)\n",
    "        plt.title('Cost per batch over the training run')\n",
    "        plt.xlabel('# batch')\n",
    "        plt.ylabel('avg. cost per batch')\n",
    "        plt.savefig(logdir+'model_'+str(ix)+'_traincost.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
