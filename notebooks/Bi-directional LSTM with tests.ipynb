{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 100000000\n",
      "(99999000, 'ons anarchists advocate social relations based upon voluntary as')\n",
      "(1000, ' anarchism originated as a term of abuse first used against earl')\n",
      "Unexpected character: ï\n",
      "(1, 26, 0, 0)\n",
      "('a', 'z', ' ')\n"
     ]
    }
   ],
   "source": [
    "# import the ptb dataset for testing\n",
    "# copy of tensorflow examples\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import string\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n",
    "\n",
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])\n",
    "\n",
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))\n",
    "\n",
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    \n",
    "    batches_ret = np.transpose(np.array(batches), (1,0,2))\n",
    "    x = batches_ret[:,0:num_unrollings,:]\n",
    "    y = batches_ret[:,1:,:]\n",
    "    \n",
    "    return x,y\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y = train_batches.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 10, 27), (64, 10, 27))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import sys, path\n",
    "sys.path.append(path.dirname(path.abspath('.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from layers import *\n",
    "\n",
    "\n",
    "# TODO: add batch re-norm and dropout\n",
    "class LSTM(object):\n",
    "    '''Class defining the overall model based on layers.py'''\n",
    "    def __init__(self, args):\n",
    "\n",
    "        self.seq_len = args['seq_len']\n",
    "        self.num_layers = args['num_layers']\n",
    "        self.cell = args['cell']\n",
    "        self.hidden_units = args['hidden_units']\n",
    "        self.ip_channels = args['ip_channels']\n",
    "        self.op_classes = args['op_channels']\n",
    "        self.mode = args['mode']\n",
    "        self.init_lr = args['lr_rate']\n",
    "        self.grad_clip = args['grad_clip']\n",
    "        self.batch_size = args['batch_size']\n",
    "\n",
    "    def build_graph(self):\n",
    "\n",
    "        self._build_model()\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            self._add_train_nodes()\n",
    "        self.summaries = tf.summary.merge_all()\n",
    "\n",
    "    def _build_model(self):\n",
    "\n",
    "        # define placeholder for data layer\n",
    "        self.input_layer_x = tf.placeholder(dtype=tf.float32, shape=[self.batch_size, self.seq_len,self.ip_channels],name='input_layer_x')\n",
    "\n",
    "        # define model based on cell and num_layers\n",
    "        if self.num_layers ==1:\n",
    "          self.lstm_layer = lstmLayer(self.hidden_units)\n",
    "        else:\n",
    "          cells = [lstmLayer(self.hidden_units)]*self.num_layers\n",
    "          self.lstm_layer = DeepLSTM(cells)\n",
    "\n",
    "        # TODO: Think about the need for statefullness in this problem scenario\n",
    "        self.initial_state = tf.zeros([self.batch_size,self.lstm_layer.state_size],tf.float32)\n",
    "        #self.initial_state = tf.placeholder(tf.float32,[None, self.lstm_layer.state_size])\n",
    "\n",
    "        state = self.initial_state\n",
    "        output = tf.zeros([self.batch_size,self.hidden_units],dtype=tf.float32)\n",
    "        outputs = []\n",
    "        # run the model for multiple time steps\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "          for time in range(self.seq_len):\n",
    "            if time > 0: tf.get_variable_scope().reuse_variables()\n",
    "            # pass the inputs, state and weather we are in train/test or inference time (for dropout)\n",
    "            output, state = self.lstm_layer(self.input_layer_x[:,time,:], state)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        self.final_output = tf.reshape(tf.concat(outputs,1),[-1,self.hidden_units])\n",
    "\n",
    "        softmax_w = tf.get_variable('softmax_w', [self.hidden_units, self.op_classes],dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer())\n",
    "        softmax_b = tf.get_variable('softmax_b', [self.op_classes],dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer())\n",
    "\n",
    "        self.logits = tf.reshape(tf.matmul(self.final_output, softmax_w) + softmax_b,\n",
    "                                 [self.batch_size,self.seq_len,self.op_classes])\n",
    "\n",
    "        # get probabilities for these logits through softmax (will be needed for sampling)\n",
    "        final_seq_op = tf.squeeze(tf.slice(self.logits,[0,self.seq_len-1,0],[-1,1,-1]))\n",
    "        self.output_prob = tf.nn.softmax(final_seq_op)\n",
    "        activation_summary(self.output_prob)\n",
    "\n",
    "    def _add_train_nodes(self):\n",
    "\n",
    "        # define placeholder for target layer\n",
    "        self.input_layer_y = tf.placeholder(tf.float32, [self.batch_size,self.seq_len,self.op_classes],name='input_layer_y')\n",
    "\n",
    "        # sequence loss by example\n",
    "        # TODO: Implement proper loss function for encoder like structure of LSTM\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=tf.concat(self.logits,0),\n",
    "                                                                           labels=tf.concat(self.input_layer_y,0)))\n",
    "        tf.summary.scalar(\"loss\",self.cost)\n",
    "\n",
    "        self.lr = tf.Variable(self.init_lr, trainable=False)\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        grads_vars = optimizer.compute_gradients(self.cost,trainable_variables)\n",
    "\n",
    "        # histogram_summaries for weights and gradients\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "        for grad, var in grads_vars:\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(var.op.name+'/gradient',grad)\n",
    "\n",
    "        # TODO: Think about how gradient clipping is implemented, cross check\n",
    "        grads, _ = tf.clip_by_global_norm([grad for (grad,var) in grads_vars], self.grad_clip)\n",
    "        self.train_op = optimizer.apply_gradients(grads_vars)\n",
    "\n",
    "    def assign_lr(self,session,lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    def initialize_state(self,session):\n",
    "        session.run(self.initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare against tensorflow implementation of BiLSTM\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell, static_bidirectional_rnn\n",
    "\n",
    "class tfBiLSTM(object):\n",
    "    \n",
    "    def __init__(self,args):\n",
    "        \n",
    "        self.n_hidden = args['hidden_units']\n",
    "        self.n_classes = args['op_channels']\n",
    "        self.batch_size = args['batch_size']\n",
    "        self.seq_len = args['seq_len']\n",
    "        self.ip_channels = args['ip_channels']\n",
    "        self.mode = args['mode']\n",
    "        self.init_lr = args['lr_rate']\n",
    "        self.op_classes = args['op_channels']\n",
    "        self.grad_clip = args['grad_clip']\n",
    "    \n",
    "    def build_graph(self):\n",
    "        \n",
    "        self._build_model()\n",
    "        if self.mode == 'train':\n",
    "            self._add_training_nodes()\n",
    "        self.summaries = tf.summary.merge_all()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \n",
    "        self.input_layer_x = tf.placeholder(dtype=tf.float32, shape=[self.batch_size, self.seq_len,self.ip_channels],name='input_layer_x')\n",
    "        \n",
    "        x = []+ tf.split(tf.transpose(self.input_layer_x, [1,0,2]), self.seq_len, axis=0)\n",
    "        x = [tf.squeeze(it) for it in x]\n",
    "        \n",
    "        weights = {'out': tf.Variable(tf.random_normal([2*self.n_hidden, self.n_classes]))}\n",
    "        biases = {'out': tf.Variable(tf.random_normal([self.n_classes]))}\n",
    "\n",
    "        # Forward direction cell\n",
    "        lstm_fw_cell = BasicLSTMCell(self.n_hidden, forget_bias=1.0)\n",
    "        # Backward direction cell\n",
    "        lstm_bw_cell = BasicLSTMCell(self.n_hidden, forget_bias=1.0)\n",
    "        outputs, _, _ = static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x, dtype=tf.float32)\n",
    "        \n",
    "        final_output = tf.reshape(tf.concat(outputs,1),[-1,2*self.n_hidden])\n",
    "        \n",
    "        self.logits = tf.reshape(tf.matmul(final_output, weights['out']) + biases['out'],\n",
    "                                 [self.batch_size,self.seq_len,self.op_classes])\n",
    "        \n",
    "        # get probabilities for these logits through softmax (will be needed for sampling)\n",
    "        final_seq_op = tf.squeeze(tf.slice(self.logits,[0,self.seq_len-1,0],[-1,1,-1]))\n",
    "        self.output_prob = tf.nn.softmax(final_seq_op)\n",
    "        activation_summary(self.output_prob)\n",
    "        \n",
    "    def _add_training_nodes(self):\n",
    "        \n",
    "        self.input_layer_y = tf.placeholder(tf.float32, [self.batch_size,self.seq_len,self.op_classes],name='input_layer_y')\n",
    "        \n",
    "        self.lr = tf.Variable(self.init_lr, trainable=False)\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=tf.concat(self.logits,0), \n",
    "                                                                        labels=tf.concat(self.input_layer_y,0)))\n",
    "        \n",
    "        \n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        grads_vars = optimizer.compute_gradients(self.cost,trainable_variables)\n",
    "\n",
    "        # histogram_summaries for weights and gradients\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "        for grad, var in grads_vars:\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(var.op.name+'/gradient',grad)\n",
    "\n",
    "        # TODO: Think about how gradient clipping is implemented, cross check\n",
    "        grads, _ = tf.clip_by_global_norm([grad for (grad,var) in grads_vars], self.grad_clip)\n",
    "        self.train_op = optimizer.apply_gradients(grads_vars)\n",
    "        \n",
    "    def assign_lr(self,session,lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bi-directional LSTM code based on LSTM code\n",
    "\n",
    "class BiLSTM(object):\n",
    "    \n",
    "    def __init__(self,args):\n",
    "        \n",
    "        self.seq_len = args['seq_len']\n",
    "        self.num_layers = args['num_layers']\n",
    "        self.cell = args['cell']\n",
    "        self.hidden_units = args['hidden_units']\n",
    "        self.op_classes = args['op_channels']\n",
    "        self.ip_channels = args['ip_channels']\n",
    "        self.mode = args['mode']\n",
    "        self.init_lr = args['lr_rate']\n",
    "        self.grad_clip = args['grad_clip']\n",
    "        self.batch_size = args['batch_size']\n",
    "        \n",
    "    def build_graph(self):\n",
    "        \n",
    "        self._build_model()\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            self._add_training_nodes()\n",
    "        self.summaries = tf.summary.merge_all()\n",
    "        \n",
    "        \n",
    "    def _build_model(self):\n",
    "        \n",
    "        # define placeholder for data layer\n",
    "        self.input_layer_x = tf.placeholder(dtype=tf.float32, shape=[self.batch_size,self.seq_len,self.ip_channels],name='input_layer_x')\n",
    "\n",
    "        # define model based on cell and num_layers\n",
    "        self.fw_lstm_layer = lstmLayer(self.hidden_units)\n",
    "        self.bk_lstm_layer = lstmLayer(self.hidden_units)\n",
    "\n",
    "        self.initial_state = tf.zeros([self.batch_size,self.fw_lstm_layer.state_size],tf.float32)\n",
    "        #self.initial_state = tf.placeholder(tf.float32,[None, self.lstm_layer.state_size])\n",
    "\n",
    "        def _run_loop(input_data,name):\n",
    "            state = self.initial_state\n",
    "            output = tf.zeros([self.batch_size,self.hidden_units],dtype=tf.float32)\n",
    "            outputs = []\n",
    "            # run the model for multiple time steps\n",
    "            with tf.variable_scope(name+\"LSTM\"):\n",
    "              for time in range(self.seq_len):\n",
    "                if time > 0: tf.get_variable_scope().reuse_variables()\n",
    "                # pass the inputs, state and weather we are in train/test or inference time (for dropout)\n",
    "                output, state = self.fw_lstm_layer(input_data[:,time,:], state)\n",
    "                outputs.append(output)\n",
    "                \n",
    "            return outputs\n",
    "        \n",
    "        # run the forward chain\n",
    "        fw_outputs = _run_loop(self.input_layer_x,'fw_')\n",
    "        # run the backward chain\n",
    "        bk_input_x = tf.reverse(self.input_layer_x, [False, True])\n",
    "        bk_outputs = _run_loop(bk_input_x,'bk_')[::-1]\n",
    "        \n",
    "        # concat the forward and backward runs\n",
    "        flat_outputs = tuple(tf.concat([fw, bw], 1) for fw, bw in zip(fw_outputs, bk_outputs))\n",
    "        self.final_output = tf.reshape(tf.concat(flat_outputs,1),[-1,2*self.hidden_units])\n",
    "        \n",
    "        # now combine with softmax to produce output\n",
    "        softmax_w = tf.get_variable('softmax_w', [2*self.hidden_units, self.op_classes],dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer())\n",
    "        softmax_b = tf.get_variable('softmax_b', [self.op_classes],dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer())\n",
    "\n",
    "        self.logits = tf.reshape(tf.matmul(self.final_output, softmax_w) + softmax_b,\n",
    "                                 [self.batch_size,self.seq_len,self.op_classes])\n",
    "\n",
    "        # get probabilities for these logits through softmax (will be needed for sampling)\n",
    "        final_seq_op = tf.squeeze(tf.slice(self.logits,[0,self.seq_len-1,0],[-1,1,-1]))\n",
    "        self.output_prob = tf.nn.softmax(final_seq_op)\n",
    "        activation_summary(self.output_prob)\n",
    "\n",
    "    def _add_training_nodes(self):\n",
    "\n",
    "        # define placeholder for target layer\n",
    "        self.input_layer_y = tf.placeholder(tf.float32, [self.batch_size,self.seq_len,self.op_classes],name='input_layer_y')\n",
    "\n",
    "        # sequence loss by example\n",
    "        # TODO: Implement proper loss function for encoder like structure of LSTM\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = tf.concat(self.logits,0),\n",
    "                                                                           labels = tf.concat(self.input_layer_y,0)))\n",
    "        tf.summary.scalar(\"loss\",self.cost)\n",
    "\n",
    "        self.lr = tf.Variable(self.init_lr, trainable=False)\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        grads_vars = optimizer.compute_gradients(self.cost,trainable_variables)\n",
    "\n",
    "        # histogram_summaries for weights and gradients\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "        for grad, var in grads_vars:\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(var.op.name+'/gradient',grad)\n",
    "\n",
    "        # TODO: Think about how gradient clipping is implemented, cross check\n",
    "        grads, _ = tf.clip_by_global_norm([grad for (grad,var) in grads_vars], self.grad_clip)\n",
    "        self.train_op = optimizer.apply_gradients(grads_vars)\n",
    "\n",
    "    def assign_lr(self,session,lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    def initialize_state(self,session):\n",
    "        session.run(self.initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-711c6bd60f74>:17: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "{'grad_clip': 5.0, 'num_layers': 1, 'batch_size': 64, 'num_epochs': 10, 'lr_rate': 0.05, 'hidden_units': 48, 'op_channels': 27, 'cell': 'lstm', 'ip_channels': 27, 'seq_len': 10, 'lr_decay': 0.97, 'mode': 'train'}\n",
      "Runtime of one epoch: \n",
      "5.70943307877\n",
      "Average cost per epoch: \n",
      "0.248844035417\n",
      "Runtime of one epoch: \n",
      "5.52858877182\n",
      "Average cost per epoch: \n",
      "0.195611312509\n",
      "Runtime of one epoch: \n",
      "5.5717921257\n",
      "Average cost per epoch: \n",
      "0.192651787773\n",
      "Runtime of one epoch: \n",
      "5.60090899467\n",
      "Average cost per epoch: \n",
      "0.190048263967\n",
      "Runtime of one epoch: \n",
      "5.68045496941\n",
      "Average cost per epoch: \n",
      "0.190043335974\n",
      "Runtime of one epoch: \n",
      "5.66954898834\n",
      "Average cost per epoch: \n",
      "0.185159908608\n",
      "Runtime of one epoch: \n",
      "5.78203892708\n",
      "Average cost per epoch: \n",
      "0.183031436875\n",
      "Runtime of one epoch: \n",
      "5.88698196411\n",
      "Average cost per epoch: \n",
      "0.184269254684\n",
      "Runtime of one epoch: \n",
      "5.65057182312\n",
      "Average cost per epoch: \n",
      "0.18285661424\n",
      "Runtime of one epoch: \n",
      "5.60437393188\n",
      "Average cost per epoch: \n",
      "0.183734913796\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAisAAAGHCAYAAABxmBIgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nw4okSSqaYUWSJBXNsCJJkor2Cd03BJq+TcZiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc7c44c35d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "args = {'num_epochs': 10, 'lr_rate': 0.05, 'lr_decay': 0.97, 'seq_len': 10, 'num_layers': 1, 'cell': 'lstm', \n",
    "        'hidden_units': 48, 'op_channels': 27, 'ip_channels': 27 ,'grad_clip': 5.0, 'batch_size': 64}\n",
    "\n",
    "# Initialize session and graph\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    \n",
    "    args['mode'] = 'train'\n",
    "\n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "\n",
    "        model = tfBiLSTM(args)\n",
    "        model.build_graph()\n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "        print args\n",
    "\n",
    "        cost_over_batches = []\n",
    "\n",
    "        for i in range(args['num_epochs']):\n",
    "            lr_decay = args['lr_decay'] ** max(i - 2.0, 0.0)\n",
    "            model.assign_lr(session, args['lr_rate'] * lr_decay)\n",
    "\n",
    "            start_time = time.time()\n",
    "            softmax_op = np.zeros((1000*model.batch_size,model.op_classes))\n",
    "            cost_trajectory = []\n",
    "            epoch_cost = 0.0\n",
    "\n",
    "            for i in range(1000):\n",
    "                x,y = train_batches.next()\n",
    "                summary, cur_cost, output_prob, _ = session.run([model.summaries,model.cost,model.output_prob,model.train_op],\n",
    "                            feed_dict={model.input_layer_x: x, model.input_layer_y: y})\n",
    "                cost_trajectory.append(cur_cost)\n",
    "                softmax_op[i*len(y):(i+1)*len(y),:] = output_prob\n",
    "                epoch_cost += cur_cost\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            print(\"Runtime of one epoch: \")\n",
    "            print(end_time-start_time)\n",
    "            print(\"Average cost per epoch: \")\n",
    "            print(epoch_cost/1000)\n",
    "\n",
    "            cost_over_batches += cost_trajectory\n",
    "\n",
    "        plt.plot(np.linspace(1,len(cost_over_batches),len(cost_over_batches)),cost_over_batches)\n",
    "        plt.title('Cost per batch over the training run')\n",
    "        plt.xlabel('# batch')\n",
    "        plt.ylabel('avg. cost per batch')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
