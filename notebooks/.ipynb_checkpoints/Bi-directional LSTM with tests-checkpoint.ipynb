{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import the ptb dataset for testing\n",
    "# copy of tensorflow examples\n",
    "\n",
    "def _read_words(filename):\n",
    "  with tf.gfile.GFile(filename, \"r\") as f:\n",
    "    return f.read().decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "\n",
    "def _build_vocab(filename):\n",
    "  data = _read_words(filename)\n",
    "\n",
    "  counter = collections.Counter(data)\n",
    "  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "  words, _ = list(zip(*count_pairs))\n",
    "  word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "  return word_to_id\n",
    "\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id):\n",
    "  data = _read_words(filename)\n",
    "  return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "\n",
    "def ptb_raw_data(data_path=None):\n",
    "  \"\"\"Load PTB raw data from data directory \"data_path\".\n",
    "  Reads PTB text files, converts strings to integer ids,\n",
    "  and performs mini-batching of the inputs.\n",
    "  The PTB dataset comes from Tomas Mikolov's webpage:\n",
    "  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "  Args:\n",
    "    data_path: string path to the directory where simple-examples.tgz has\n",
    "      been extracted.\n",
    "  Returns:\n",
    "    tuple (train_data, valid_data, test_data, vocabulary)\n",
    "    where each of the data objects can be passed to PTBIterator.\n",
    "  \"\"\"\n",
    "\n",
    "  train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "  valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "  test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "\n",
    "  word_to_id = _build_vocab(train_path)\n",
    "  train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "  valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "  test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "  vocabulary = len(word_to_id)\n",
    "  return train_data, valid_data, test_data, vocabulary\n",
    "\n",
    "\n",
    "def ptb_producer(raw_data, batch_size, num_steps, name=None):\n",
    "  \"\"\"Iterate on the raw PTB data.\n",
    "  This chunks up raw_data into batches of examples and returns Tensors that\n",
    "  are drawn from these batches.\n",
    "  Args:\n",
    "    raw_data: one of the raw data outputs from ptb_raw_data.\n",
    "    batch_size: int, the batch size.\n",
    "    num_steps: int, the number of unrolls.\n",
    "    name: the name of this operation (optional).\n",
    "  Returns:\n",
    "    A pair of Tensors, each shaped [batch_size, num_steps]. The second element\n",
    "    of the tuple is the same data time-shifted to the right by one.\n",
    "  Raises:\n",
    "    tf.errors.InvalidArgumentError: if batch_size or num_steps are too high.\n",
    "  \"\"\"\n",
    "  with tf.name_scope(name, \"PTBProducer\", [raw_data, batch_size, num_steps]):\n",
    "    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n",
    "\n",
    "    data_len = tf.size(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    data = tf.reshape(raw_data[0 : batch_size * batch_len],\n",
    "                      [batch_size, batch_len])\n",
    "\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "    assertion = tf.assert_positive(\n",
    "        epoch_size,\n",
    "        message=\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "    with tf.control_dependencies([assertion]):\n",
    "      epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\n",
    "\n",
    "    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n",
    "    x = tf.strided_slice(data, [0, i * num_steps], [-1,-1],\n",
    "                         [batch_size, (i + 1) * num_steps])\n",
    "    x.set_shape([batch_size, num_steps])\n",
    "    y = tf.strided_slice(data, [0, i * num_steps + 1], [-1,-1],\n",
    "                         [batch_size, (i + 1) * num_steps + 1])\n",
    "    y.set_shape([batch_size, num_steps])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, val, test, vocab = ptb_raw_data('/home/kaushik/Desktop/timeseriesDL/data/ptb')\n",
    "\n",
    "x_batch, y_batch = ptb_producer(train, 64, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'PTBProducer_2/StridedSlice:0' shape=(64, 20) dtype=int32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: add batch re-norm and dropout\n",
    "class LSTM(object):\n",
    "    '''Class defining the overall model based on layers.py'''\n",
    "    def __init__(self, args):\n",
    "\n",
    "        self.seq_len = args['seq_len']\n",
    "        self.num_layers = args['num_layers']\n",
    "        self.cell = args['cell']\n",
    "        self.hidden_units = args['hidden_units']\n",
    "        self.ip_channels = args['ip_channels']\n",
    "        self.op_classes = args['op_channels']\n",
    "        self.mode = args['mode']\n",
    "        self.init_lr = args['lr_rate']\n",
    "        self.grad_clip = args['grad_clip']\n",
    "        self.batch_size = args['batch_size']\n",
    "        self.class_weights = args['weights']\n",
    "\n",
    "    def build_graph(self):\n",
    "\n",
    "        self._build_model()\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            self._add_train_nodes()\n",
    "        self.summaries = tf.summary.merge_all()\n",
    "\n",
    "    def _build_model(self):\n",
    "\n",
    "        # define placeholder for data layer\n",
    "        self.input_layer_x = tf.placeholder(dtype=tf.float32, shape=[self.batch_size, self.seq_len, self.ip_channels],name='input_layer_x')\n",
    "\n",
    "        # define model based on cell and num_layers\n",
    "        if self.num_layers ==1:\n",
    "          self.lstm_layer = lstmLayer(self.hidden_units)\n",
    "        else:\n",
    "          cells = [lstmLayer(self.hidden_units)]*self.num_layers\n",
    "          self.lstm_layer = DeepLSTM(cells)\n",
    "\n",
    "        # TODO: Think about the need for statefullness in this problem scenario\n",
    "        self.initial_state = tf.zeros([self.batch_size,self.lstm_layer.state_size],tf.float32)\n",
    "        #self.initial_state = tf.placeholder(tf.float32,[None, self.lstm_layer.state_size])\n",
    "\n",
    "        state = self.initial_state\n",
    "        output = tf.zeros([self.batch_size,self.hidden_units],dtype=tf.float32)\n",
    "        # run the model for multiple time steps\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "          for time in range(self.seq_len):\n",
    "            if time > 0: tf.get_variable_scope().reuse_variables()\n",
    "            # pass the inputs, state and weather we are in train/test or inference time (for dropout)\n",
    "            output, state = self.lstm_layer(self.input_layer_x[:,time,:], state)\n",
    "\n",
    "        self.final_state = state\n",
    "        self.final_output = output\n",
    "\n",
    "        softmax_w = tf.get_variable('softmax_w', [self.hidden_units, self.op_classes],dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer())\n",
    "        softmax_b = tf.get_variable('softmax_b', [self.op_classes],dtype=tf.float32,\n",
    "                                    initializer=tf.random_uniform_initializer())\n",
    "\n",
    "        self.logits = tf.matmul(self.final_output, softmax_w) + softmax_b\n",
    "\n",
    "        # get probabilities for these logits through softmax (will be needed for sampling)\n",
    "        self.output_prob = tf.nn.softmax(self.logits)\n",
    "        activation_summary(self.output_prob)\n",
    "\n",
    "    def _add_train_nodes(self):\n",
    "\n",
    "        # define placeholder for target layer\n",
    "        self.input_layer_y = tf.placeholder(tf.float32, [self.batch_size,self.op_classes],name='input_layer_y')\n",
    "\n",
    "        # sequence loss by example\n",
    "        # TODO: Implement proper loss function for encoder like structure of LSTM\n",
    "        #self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.logits,self.input_layer_y))\n",
    "        self.cost = weighted_cross_entropy(self.class_weights,self.logits,self.input_layer_y)\n",
    "        tf.summary.scalar(\"loss\",self.cost)\n",
    "\n",
    "        self.lr = tf.Variable(self.init_lr, trainable=False)\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        grads_vars = optimizer.compute_gradients(self.cost,trainable_variables)\n",
    "\n",
    "        # histogram_summaries for weights and gradients\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.op.name, var)\n",
    "        for grad, var in grads_vars:\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(var.op.name+'/gradient',grad)\n",
    "\n",
    "        # TODO: Think about how gradient clipping is implemented, cross check\n",
    "        grads, _ = tf.clip_by_global_norm([grad for (grad,var) in grads_vars], self.grad_clip)\n",
    "        self.train_op = optimizer.apply_gradients(grads_vars)\n",
    "\n",
    "    def assign_lr(self,session,lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    def initialize_state(self,session):\n",
    "        session.run(self.initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named timeseriesDL.models",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-311721389070>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Bi-directional LSTM code based on LSTM code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtimeseriesDL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: No module named timeseriesDL.models"
     ]
    }
   ],
   "source": [
    "# Bi-directional LSTM code based on LSTM code\n",
    "\n",
    "class BiLSTM(object):\n",
    "    \n",
    "    def __init__(self,args):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def _build_model(self):\n",
    "        \n",
    "        # define placeholder for data layer\n",
    "        self.input_layer_x = tf.placeholder(dtype=tf.int32, shape=[self.batch_size, self.seq_len],name='input_layer_x')\n",
    "\n",
    "        # define model based on cell and num_layers\n",
    "        self.fw_lstm_layer = lstmLayer(self.hidden_units)\n",
    "        self.bk_lstm_layer = lstmLayer(self.hidden_units)\n",
    "\n",
    "        self.initial_state = tf.zeros([self.batch_size,self.fw_lstm_layer.state_size],tf.float32)\n",
    "        #self.initial_state = tf.placeholder(tf.float32,[None, self.lstm_layer.state_size])\n",
    "\n",
    "        def _run_loop(input_data,name):\n",
    "            state = self.initial_state\n",
    "            output = tf.zeros([self.batch_size,self.hidden_units],dtype=tf.float32)\n",
    "            outputs = []\n",
    "            # run the model for multiple time steps\n",
    "            with tf.variable_scope(name+\"LSTM\"):\n",
    "              for time in range(self.seq_len):\n",
    "                if time > 0: tf.get_variable_scope().reuse_variables()\n",
    "                # pass the inputs, state and weather we are in train/test or inference time (for dropout)\n",
    "                output, state = self.fw_lstm_layer(input_data[:,time,:], state)\n",
    "                outputs.append(output)\n",
    "                \n",
    "            return outputs\n",
    "        \n",
    "        # run the forward chain\n",
    "        fw_outputs = _run_loop(self.input_layer_x,'fw_')\n",
    "        # run the backward chain\n",
    "        bk_input_x = tf.reverse(self.input_layer_x, [False, True])\n",
    "        bk_ouputs = _run_loop(bk_input_x,'bk_')[::-1]\n",
    "        \n",
    "        \n",
    "        # concat the forward and backward runs\n",
    "        final_ouput = tf.zeros([self.seq_len,self.batch_size,self.hidden_units],dtype=tf.float32)\n",
    "        for it, (fw,bk) in enumerate(zip(fw_outputs,bk_outputs)):\n",
    "            final_ouput[it,:,:] = tf.concat(1,[fw,bk])\n",
    "        \n",
    "        \n",
    "        return final_output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
